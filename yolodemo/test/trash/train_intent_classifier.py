
import os
import random
import joblib
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# æ§åˆ¶ç±»è¯­æ–™ï¼ˆå¯æ‰©å±•ï¼‰
control_templates = {
    "ä¼¸å‡ºæœºæ¢°è‡‚": ["ä¼¸å‡ºæœºæ¢°è‡‚", "æŠŠæœºæ¢°è‡‚ä¼¸å‡ºæ¥", "å±•å¼€æœºæ¢°è‡‚", "æœºæ¢°è‡‚å‰è¿›", "æœºæ¢°è‡‚ä¼¸å‡ºå»"],
    "æ”¶å›æœºæ¢°è‡‚": ["æ”¶å›æœºæ¢°è‡‚", "å…³é—­æœºæ¢°è‡‚", "æœºæ¢°è‡‚æ”¶å›æ¥", "æœºæ¢°è‡‚åé€€", "æœºæ¢°è‡‚é€€å›å»"],
    "ä¼¸å‡ºä¼ æ„Ÿå™¨": ["ä¼¸å‡ºä¼ æ„Ÿå™¨", "å±•å¼€ä¼ æ„Ÿå™¨", "æŠŠä¼ æ„Ÿå™¨ä¼¸å‡ºæ¥", "å‰å‡ºä¼ æ„Ÿå™¨", "ä¼ æ„Ÿå™¨å¯åŠ¨"],
    "æ”¶å›ä¼ æ„Ÿå™¨": ["æ”¶å›ä¼ æ„Ÿå™¨", "å…³é—­ä¼ æ„Ÿå™¨", "æ’¤å›ä¼ æ„Ÿå™¨", "åé€€ä¼ æ„Ÿå™¨", "ç»“æŸä¼ æ„Ÿå™¨å·¥ä½œ"],
    "å¼€å§‹æµ‹é‡æº¶æ°§": ["å¼€å§‹æµ‹é‡æº¶æ°§", "å¯åŠ¨æº¶æ°§æ£€æµ‹", "æ£€æµ‹æº¶æ°§", "æµ‹æ°§", "æµ‹æº¶æ°§"],
    "åœæ­¢æµ‹é‡æº¶æ°§": ["åœæ­¢æµ‹é‡æº¶æ°§", "å…³é—­æº¶æ°§", "ç»ˆæ­¢æº¶æ°§æ£€æµ‹", "å–æ¶ˆæº¶æ°§", "æº¶æ°§æ£€æµ‹ç»“æŸ"],
    "å¼€å§‹æµ‹é‡PHå€¼": ["å¼€å§‹æµ‹é‡PH", "æ£€æµ‹PHå€¼", "å¯åŠ¨PHä¼ æ„Ÿå™¨", "PHå€¼å¼€å§‹æ£€æµ‹", "å¼€å§‹PHæµ‹é‡"],
    "åœæ­¢æµ‹é‡PHå€¼": ["åœæ­¢æµ‹é‡PH", "ç»ˆæ­¢PHæ£€æµ‹", "å…³é—­PHä¼ æ„Ÿå™¨", "ç»“æŸPHæ£€æµ‹", "PHå€¼æµ‹å®Œäº†"],
    "æ‰“å¼€ç›¸æœº": ["æ‰“å¼€ç›¸æœº", "å¯åŠ¨ç›¸æœº", "å¼€å¯æ‘„åƒå¤´", "ç›¸æœºå·¥ä½œ", "æ‰“å¼€æ‘„åƒ"],
    "å¼€å§‹æ‹ç…§": ["å¼€å§‹æ‹ç…§", "æ‹ä¸€å¼ ", "æ‰§è¡Œæ‹æ‘„", "è¿›è¡Œæ‹ç…§", "æ‹ä¸ªç…§ç‰‡"],
    "å…³é—­ç›¸æœº": ["å…³é—­ç›¸æœº", "å…³é—­æ‘„åƒå¤´", "åœæ­¢æ‹æ‘„", "ç›¸æœºå…³æ‰", "ç»“æŸæ‘„åƒ"]
}

# èŠå¤©è¯­æ–™ï¼ˆæ›´ä¸°å¯Œï¼‰
chat_examples = [
    "ä½ æ˜¯è°", "ä½ å«ä»€ä¹ˆ", "ä»Šå¤©å¤©æ°”å¥½å—", "æˆ‘ä»¬èŠèŠ", "ä½ ä¼šå¹²ä»€ä¹ˆ", "ä½ çœŸèªæ˜", "å†è§", "ä½ åœ¨å¹²å˜›", "è®²ä¸ªç¬‘è¯",
    "ä½ å¥½", "ä½ å–œæ¬¢åƒä»€ä¹ˆ", "ä½ æ˜¯æœºå™¨äººå—", "ä½ å¤šå¤§", "ä½ çœŸæ£’", "ä½ å¯ä»¥è¯´è¯å—", "ä½ å¬å¾—åˆ°å—", "ä½ å«ä»€ä¹ˆåå­—",
    "æˆ‘èƒ½é—®ä½ é—®é¢˜å—", "ä½ å‡ ç‚¹ç¡è§‰", "ä½ å–œæ¬¢ä»€ä¹ˆé¢œè‰²", "ä½ å–œæ¬¢æ—…è¡Œå—", "ä½ å–œæ¬¢èŠå¤©å—", "æˆ‘ä»¬è¯´ç‚¹åˆ«çš„", "æˆ‘æƒ³é—®ä½ ç‚¹äº‹"
]

# æ‰©å±•æ§åˆ¶ç±»è¯­æ–™ï¼ˆæ¯ä¸ªè¡¨è¾¾é‡å¤4æ¬¡ï¼‰
control_texts = []
for phrases in control_templates.values():
    for p in phrases:
        for _ in range(4):
            control_texts.append(p)

# æ‰©å±•èŠå¤©è¯­æ–™ï¼ˆæ¯å¥é‡å¤10æ¬¡ï¼‰
chat_texts = [s for s in chat_examples for _ in range(10)]

# åˆå¹¶æ ·æœ¬
texts = control_texts + chat_texts
labels = ["control"] * len(control_texts) + ["chat"] * len(chat_texts)

# æ‰“ä¹±é¡ºåº
combined = list(zip(texts, labels))
random.shuffle(combined)
texts, labels = zip(*combined)

# åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.2, random_state=42)

# å‘é‡åŒ– + æ¨¡å‹è®­ç»ƒï¼ˆè®¾ç½® max_iter=20000ï¼‰
vectorizer = TfidfVectorizer()
X_train_vec = vectorizer.fit_transform(X_train)
X_test_vec = vectorizer.transform(X_test)

clf = LogisticRegression(max_iter=20000, solver='lbfgs', class_weight='balanced', random_state=42)
clf.fit(X_train_vec, y_train)

# æ¨¡å‹ä¿å­˜
os.makedirs("intent_model_trained", exist_ok=True)
joblib.dump(clf, "intent_model_trained/intent_classifier.pkl")
joblib.dump(vectorizer, "intent_model_trained/vectorizer.pkl")

# ä¿å­˜è¯­æ–™
with open("intent_model_trained/train_corpus.txt", "w", encoding="utf-8") as f:
    for text, label in zip(texts, labels):
        f.write(f"{label}\t{text}\n")

# è¾“å‡ºé¢„æµ‹æŠ¥å‘Š
y_pred = clf.predict(X_test_vec)
print("ğŸ“Š åˆ†ç±»æŠ¥å‘Š:")
print(classification_report(y_test, y_pred))

# å¯è§†åŒ–æ··æ·†çŸ©é˜µ
cm = confusion_matrix(y_test, y_pred, labels=["control", "chat"])
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt="d", xticklabels=["control", "chat"], yticklabels=["control", "chat"], cmap="Blues")
plt.title("æ··æ·†çŸ©é˜µ")
plt.xlabel("é¢„æµ‹")
plt.ylabel("å®é™…")
plt.tight_layout()
plt.savefig("intent_model_trained/confusion_matrix.png")
print("\nâœ… æ¨¡å‹è®­ç»ƒå®Œæˆï¼Œå·²ä¿å­˜è‡³ intent_model_trained/")
