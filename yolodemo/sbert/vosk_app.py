import os
import queue
import json
import threading
import time

import numpy as np
import sounddevice as sd
import pyttsx3
import joblib
import cv2
from PyQt5.QtWidgets import (
    QApplication, QWidget, QPushButton, QTextEdit, QVBoxLayout, QLabel, QDesktopWidget
)
from PyQt5.QtGui import QImage, QPixmap
from PyQt5.QtCore import Qt, QTimer
from sentence_transformers import SentenceTransformer
from vosk import Model, KaldiRecognizer
from sklearn.metrics.pairwise import cosine_similarity

from sbert.sbert_const import local_huggingface_path

q = queue.Queue()
is_listening = False
rec = None

intent_to_natural_reply = {
    "extend_arm": "è½¬åŠ¨æœºæ¢°è‡‚",
    "retract_arm": "æ”¶å›æœºæ¢°è‡‚",
    "extend_sensor": "ä¼¸å‡ºä¼ æ„Ÿå™¨",
    "retract_sensor": "æ”¶å›ä¼ æ„Ÿå™¨",
    "start_oxygen": "å¼€å§‹æµ‹é‡æº¶æ°§",
    "stop_oxygen": "åœæ­¢æµ‹é‡æº¶æ°§",
    "start_ph": "å¼€å§‹æµ‹é‡PHå€¼",
    "stop_ph": "åœæ­¢PHæ£€æµ‹",
    "open_camera": "æ‰“å¼€ç›¸æœº",
    "take_photo": "æ‹ç…§",
    "close_camera": "å…³é—­ç›¸æœº",
}

vosk_model_path = "../model/vosk/vosk-model-cn-0.22"
#vosk_model_path = "../../model/vosk/vosk-model-small-cn-0.22"

if not os.path.exists(vosk_model_path):
    raise FileNotFoundError("âŒ ç¼ºå°‘ Vosk ä¸­æ–‡æ¨¡å‹ï¼Œè¯·ä¸‹è½½è§£å‹åæ”¾ç½®åœ¨å½“å‰ç›®å½•")
rec = KaldiRecognizer(Model(vosk_model_path), 16000)

sbert_index = joblib.load("sbert_intent/intent_sbert_index.pkl")
sbert_model = SentenceTransformer(local_huggingface_path, local_files_only=True)
index_vecs = sbert_index["embeddings"]
index_labels = sbert_index["labels"]
index_texts = sbert_index["texts"]

engine = pyttsx3.init()
engine.setProperty("rate", 160)
engine.setProperty("volume", 1.0)

def short_reply(text):
    if "ä½ æ˜¯è°" in text:
        return "æˆ‘æ˜¯è¯­éŸ³åŠ©æ‰‹"
    elif "å¤©æ°”" in text:
        return "æˆ‘ä¸æŸ¥å¤©æ°”"
    elif "ä½ å¥½" in text:
        return "ä½ å¥½ï¼"
    else:
        return "ä½ è¯´çš„æ˜¯ï¼š" + text[:10]

def predict_intent(text, threshold=0.6):
    vec = sbert_model.encode([text])
    sims = cosine_similarity(vec, index_vecs)[0]
    best_idx = np.argmax(sims)
    best_score = sims[best_idx]
    best_label = index_labels[best_idx]
    best_template = index_texts[best_idx]
    if best_score < threshold:
        return "chat", best_score, best_template
    return best_label, best_score, best_template

def speak_async(text):
    print(text)
    # threading.Thread(target=lambda: engine.say(text) or engine.runAndWait(), daemon=True).start()

def audio_callback(indata, frames, time, status):
    if is_listening:
        q.put(bytes(indata))

class VoiceApp(QWidget):
    def __init__(self):
        super().__init__()

        self.timer = QTimer(self)
        self.timer.timeout.connect(self.update_camera_frame)
        self.cap = None

        self.setWindowTitle("è¯­éŸ³åŠ©æ‰‹")
        self.resize(800, 600)
        self.center_window()

        self.partial_label = QLabel("", self)
        self.partial_label.setStyleSheet("color: gray; font-size: 18px; font-style: italic;")

        self.text_display = QTextEdit(self)
        self.text_display.setReadOnly(True)
        self.text_display.setStyleSheet("font-size: 16px;")

        self.video_label = QLabel(self)
        self.video_label.setFixedSize(640, 480)
        self.video_label.setStyleSheet("background-color: black")

        self.button = QPushButton("ğŸ™ï¸ å¼€å§‹", self)
        self.button.setStyleSheet("font-size: 16px;")
        self.button.clicked.connect(self.toggle_recognition)

        self.open_camera_button = QPushButton("ğŸ“· æ‰“å¼€ç›¸æœº", self)
        self.open_camera_button.setStyleSheet("font-size: 16px;")
        self.open_camera_button.clicked.connect(self.show_camera_preview)

        layout = QVBoxLayout()
        layout.addWidget(self.partial_label)
        layout.addWidget(self.text_display)
        layout.addWidget(self.video_label)
        layout.addWidget(self.button)
        layout.addWidget(self.open_camera_button)
        self.setLayout(layout)

        self.stream = sd.RawInputStream(
            samplerate=16000,
            blocksize=4000,
            dtype='int16',
            channels=1,
            callback=audio_callback,
            latency='low'
        )

    def execute_action(self, label):
        if label == "take_photo":
            QTimer.singleShot(0, self.take_photo_with_camera_check)
        elif label == "open_camera":
            QTimer.singleShot(0, self.show_camera_preview)
        elif label == "close_camera":
            QTimer.singleShot(0, self.stop_camera_preview)
        speak_async(f"æ‰§è¡Œ {label}")

    def take_photo_with_camera_check(self):
        if self.cap is None or not self.cap.isOpened():
            self.show_camera_preview(then_capture=True)
        else:
            self.capture_photo()

    def center_window(self):
        screen = QDesktopWidget().screenGeometry()
        size = self.geometry()
        self.move((screen.width() - size.width()) // 2,
                  (screen.height() - size.height()) // 2)

    def toggle_recognition(self):
        global is_listening
        if not is_listening:
            self.text_display.clear()
            self.partial_label.setText("")
            self.button.setText("â¹ï¸ åœæ­¢")
            is_listening = True
            self.stream.start()
            threading.Thread(target=self.recognition_loop, daemon=True).start()
        else:
            is_listening = False
            self.button.setText("ğŸ™ï¸ å¼€å§‹")
            self.stream.stop()
            while not q.empty():
                try:
                    q.get_nowait()
                except queue.Empty:
                    break
            try:
                final_result = json.loads(rec.FinalResult())
                text = final_result.get("text", "").strip()
                if text:
                    self.handle_text(text)
            except Exception as e:
                self.text_display.append(f"âš ï¸ å°¾éŸ³è¯†åˆ«å¤±è´¥ï¼š{e}")

    def recognition_loop(self):
        last_partial = ""
        last_partial_time = time.time()
        while is_listening:
            try:
                data = q.get(timeout=1)
            except queue.Empty:
                continue
            if rec.AcceptWaveform(data):
                result = json.loads(rec.Result())
                text = result.get("text", "").strip()
                self.partial_label.setText("")
                if text:
                    self.handle_text(text)
            else:
                partial = json.loads(rec.PartialResult())
                ptext = partial.get("partial", "")
                now = time.time()
                if ptext and ptext != last_partial and now - last_partial_time > 0.3:
                    self.partial_label.setText(f"ğŸ•“ å½“å‰è¯†åˆ«ï¼š{ptext}")
                    last_partial = ptext
                    last_partial_time = now

    def handle_text(self, text):
        if len(text) < 2:
            print(f"\nè¯†åˆ«ç»“æœå¤ªçŸ­ï¼š{text}")
            return
        self.text_display.append(f"\nâœ… æœ€ç»ˆè¯†åˆ«ç»“æœï¼š{text}")
        label, score, matched = predict_intent(text)
        self.text_display.append(f"ğŸ¯ æ„å›¾ï¼š{label}ï¼ˆç½®ä¿¡åº¦ï¼š{score:.2f}ï¼‰")
        if label == "chat":
            reply = short_reply(text)
            self.text_display.append(f"ğŸ’¬ å›å¤ï¼š{reply}")
            speak_async(reply)
        else:
            reply_text = intent_to_natural_reply.get(label, matched)
            friendly_reply = f"å¥½çš„ï¼Œæˆ‘é©¬ä¸Š{reply_text}"
            self.text_display.append(f"ğŸ’¬ å›å¤ï¼š{friendly_reply}")
            speak_async(friendly_reply)
            self.execute_action(label)
        rec.Reset()

    def update_camera_frame(self):
        if self.cap and self.cap.isOpened():
            ret, frame = self.cap.read()
            if ret:
                frame = cv2.resize(frame, (640, 480))
                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                h, w, ch = frame.shape
                bytes_per_line = ch * w
                qimg = QImage(frame.data, w, h, bytes_per_line, QImage.Format_RGB888)
                pixmap = QPixmap.fromImage(qimg)
                self.video_label.setPixmap(pixmap)
            else:
                print("âš ï¸ cap.read() æ— æ³•è·å–å¸§")
        else:
            print("âš ï¸ ç›¸æœºæœªå¼€å¯æˆ–è¯»å–å¤±è´¥")

    def show_camera_preview(self, then_capture=False):
        print("ğŸ¥ show_camera_preview è°ƒç”¨")
        if self.cap is not None and self.cap.isOpened():
            print("ğŸ“¸ ç›¸æœºå·²åœ¨è¿è¡Œä¸­")
            return

        self.cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)
        if not self.cap.isOpened():
            self.text_display.append("âŒ ç›¸æœºæ‰“å¼€å¤±è´¥")
            return

        print("âœ… ç›¸æœºæˆåŠŸæ‰“å¼€")
        self.text_display.append("âœ… ç›¸æœºé¢„è§ˆå·²å¯åŠ¨")
        self.timer.start(30)

        if then_capture:
            QTimer.singleShot(1500, self.capture_photo)

    def capture_photo(self):
        print('ğŸ“¸ capture_photo è°ƒç”¨')
        if self.cap and self.cap.isOpened():
            ret, frame = self.cap.read()
            if ret:
                timestamp = time.strftime("%Y%m%d_%H%M%S")
                filename = f"{timestamp}.jpg"
                cv2.imwrite(filename, frame)
                self.text_display.append(f"ğŸ“¸ å·²ä¿å­˜ç…§ç‰‡ä¸ºï¼š{filename}")
            else:
                print("âš ï¸ æ‹ç…§å¤±è´¥ï¼šæ— æ³•è¯»å–å¸§")
        else:
            print("âš ï¸ æ‹ç…§å¤±è´¥ï¼šç›¸æœºæœªæ‰“å¼€")

    def stop_camera_preview(self):
        print("ğŸ“´ stop_camera_preview")
        if self.cap:
            self.timer.stop()
            self.cap.release()
            self.cap = None
            self.video_label.clear()
            self.text_display.append("ğŸ“· ç›¸æœºå·²å…³é—­")

if __name__ == "__main__":
    import sys
    app = QApplication(sys.argv)
    window = VoiceApp()
    window.show()
    sys.exit(app.exec_())
