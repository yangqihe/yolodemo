import sys
import time
import queue
import threading
import numpy as np
import pycorrector
import sounddevice as sd
import soundfile as sf
import pyttsx3
import joblib
import cv2
import whisper
from PyQt5.QtWidgets import (
    QApplication, QWidget, QPushButton, QTextEdit, QVBoxLayout, QLabel, QDesktopWidget
)
from PyQt5.QtGui import QImage, QPixmap
from PyQt5.QtCore import Qt, QTimer
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

from sbert.sbert_const import local_model_path, predict_threshold, control_templates
from sbert.voice_app import intent_to_natural_reply
from opencc import OpenCC
import torch
import re
import difflib
from pypinyin import lazy_pinyin

q = queue.Queue()
is_listening = False

# intent_to_natural_reply = {
#     "extend_arm": "è½¬åŠ¨æœºæ¢°è‡‚",
#     "retract_arm": "æ”¶å›æœºæ¢°è‡‚",
#     "extend_sensor": "ä¼¸å‡ºä¼ æ„Ÿå™¨",
#     "retract_sensor": "æ”¶å›ä¼ æ„Ÿå™¨",
#     "start_oxygen": "å¼€å§‹æµ‹é‡æº¶æ°§",
#     "stop_oxygen": "åœæ­¢æµ‹é‡æº¶æ°§",
#     "start_ph": "å¼€å§‹æµ‹é‡PHå€¼",
#     "stop_ph": "åœæ­¢PHæ£€æµ‹",
#     "open_camera": "æ‰“å¼€ç›¸æœº",
#     "take_photo": "æ‹ç…§",
#     "close_camera": "å…³é—­ç›¸æœº",
# }

# å±•å¹³æ¨¡æ¿å¥å­
all_templates = [phrase for phrases in control_templates.values() for phrase in phrases]

sbert_index = joblib.load("sbert_intent/intent_sbert_index.pkl")
# sbert_model = SentenceTransformer(sbert_index["model_name"])
sbert_model = SentenceTransformer(local_model_path, local_files_only=True)
index_vecs = sbert_index["embeddings"]
index_labels = sbert_index["labels"]
index_texts = sbert_index["texts"]

engine = pyttsx3.init()
engine.setProperty("rate", 160)
engine.setProperty("volume", 1.0)

# model = whisper.load_model("medium")
if torch.cuda.is_available():
    model = whisper.load_model("medium").to("cuda")
    print("âœ… ä½¿ç”¨ GPU æ¨ç†")
else:
    model = whisper.load_model("medium")
    print("âš ï¸ æœªæ£€æµ‹åˆ° GPUï¼Œä½¿ç”¨ CPU æ¨ç†")


def predict_intent(text, threshold=predict_threshold):
    vec = sbert_model.encode([text])
    sims = cosine_similarity(vec, index_vecs)[0]
    best_idx = np.argmax(sims)
    best_score = sims[best_idx]
    best_label = index_labels[best_idx]
    best_template = index_texts[best_idx]
    if best_score < threshold:
        return "chat", best_score, best_template
    return best_label, best_score, best_template


def speak_async(text):
    threading.Thread(target=lambda: engine.say(text) or engine.runAndWait(), daemon=True).start()


class WhisperVoiceApp(QWidget):
    def __init__(self):
        super().__init__()

        self.setWindowTitle("Whisper ä¸­æ–‡è¯­éŸ³è¯†åˆ« + æ§åˆ¶æ‰§è¡Œ")
        self.resize(800, 600)
        self.center_window()

        self.cap = None
        self.timer = QTimer()
        self.timer.timeout.connect(self.update_camera_frame)

        self.status_label = QLabel("ç‚¹å‡»æŒ‰é’®å¼€å§‹è¯†åˆ«", self)
        self.status_label.setAlignment(Qt.AlignCenter)

        self.video_label = QLabel(self)
        self.video_label.setFixedSize(640, 480)
        self.video_label.setStyleSheet("background-color: black")

        self.text_display = QTextEdit(self)
        self.text_display.setReadOnly(True)

        self.button = QPushButton("ğŸ™ï¸ å½•éŸ³è¯†åˆ«æŒ‡ä»¤", self)
        self.button.clicked.connect(self.start_recognition)

        layout = QVBoxLayout()
        layout.addWidget(self.status_label)
        layout.addWidget(self.video_label)
        layout.addWidget(self.text_display)
        layout.addWidget(self.button)
        self.setLayout(layout)

    def center_window(self):
        screen = QDesktopWidget().screenGeometry()
        size = self.geometry()
        self.move((screen.width() - size.width()) // 2,
                  (screen.height() - size.height()) // 2)

    def start_recognition(self):
        # è®¾ç½®æŒ‰é’®ä¸å¯ç‚¹å‡» + æ”¹å˜æ–‡å­—
        self.button.setEnabled(False)
        self.button.setText("ğŸ§ å½•éŸ³ä¸­...")
        self.text_display.clear()
        self.status_label.setText("ğŸ¤ å¼€å§‹å½•éŸ³...")
        QApplication.processEvents()

        duration = 3
        samplerate = 16000
        audio = sd.rec(int(samplerate * duration), samplerate=samplerate, channels=1, dtype='int16')

        for i in range(duration, 0, -1):
            self.status_label.setText(f"ğŸ™ï¸ æ­£åœ¨å½•éŸ³ï¼ˆå‰©ä½™ {i} ç§’ï¼‰...")
            QApplication.processEvents()
            time.sleep(1)

        sd.wait()
        sf.write("temp.wav", audio, samplerate)

        self.status_label.setText("ğŸ§  æ­£åœ¨è¯†åˆ«ä¸­...")
        QApplication.processEvents()

        result = model.transcribe("temp.wav", language="zh")
        text = OpenCC('t2s').convert(result['text'].strip())

        self.text_display.append(f"âœ… è¯†åˆ«ç»“æœï¼š{text}")
        corrected, score = self.get_best_pinyin_match(text, all_templates)
        self.text_display.append(f"åŸå¥: {text} => çº æ­£: {corrected}ï¼ˆç›¸ä¼¼åº¦: {score:.2f}ï¼‰")
        text = corrected
        if len(text) < 2:
            return

        label, score, matched = predict_intent(text)
        self.text_display.append(f"ğŸ“¦ æŒ‡ä»¤è¯†åˆ«ï¼š{label} ({score:.2f})")

        if label == "chat":
            reply = "ä½ è¯´çš„æ˜¯ï¼š" + text[:10]
        else:
            reply = f"å¥½çš„ï¼Œæˆ‘é©¬ä¸Š{intent_to_natural_reply.get(label, label)}"
            self.execute_action(label)

        self.text_display.append(f"ğŸ’¬ å›å¤ï¼š{reply}")
        speak_async(reply)

        self.status_label.setText("âœ… è¯†åˆ«å®Œæˆ")
        # æ¢å¤æŒ‰é’®
        self.button.setEnabled(True)
        self.button.setText("ğŸ™ï¸ å½•éŸ³è¯†åˆ«æŒ‡ä»¤")

    def get_best_pinyin_match(self,input_text, templates, threshold = predict_threshold):
        input_pinyin = lazy_pinyin(input_text)
        best_match = None
        best_score = 0.0

        for template in templates:
            template_pinyin = lazy_pinyin(template)
            matcher = difflib.SequenceMatcher(None, input_pinyin, template_pinyin)
            score = matcher.ratio()
            if score > best_score:
                best_match = template
                best_score = score

        return (best_match, best_score) if best_score >= threshold else (input_text, best_score)

    def clean_text(self, text):
        return re.sub(r"[^\u4e00-\u9fa5a-zA-Z0-9ï¼Œã€‚ï¼ï¼Ÿã€â€œâ€]", "", text)

    def execute_action(self, label):
        if label == "take_photo":
            QTimer.singleShot(0, self.take_photo_with_camera_check)
        elif label == "open_camera":
            QTimer.singleShot(0, self.show_camera_preview)
        elif label == "close_camera":
            QTimer.singleShot(0, self.stop_camera_preview)

    def update_camera_frame(self):
        if self.cap and self.cap.isOpened():
            ret, frame = self.cap.read()
            if ret:
                frame = cv2.resize(frame, (640, 480))
                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                qimg = QImage(frame.data, frame.shape[1], frame.shape[0],
                              frame.strides[0], QImage.Format_RGB888)
                self.video_label.setPixmap(QPixmap.fromImage(qimg))

    def show_camera_preview(self, then_capture=False):
        self.cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)
        if self.cap.isOpened():
            self.text_display.append("ğŸ“· ç›¸æœºå·²æ‰“å¼€")
            self.timer.start(30)
            if then_capture:
                QTimer.singleShot(1500, self.capture_photo)

    def take_photo_with_camera_check(self):
        if not self.cap or not self.cap.isOpened():
            self.show_camera_preview(then_capture=True)
        else:
            self.capture_photo()

    def capture_photo(self):
        if self.cap and self.cap.isOpened():
            ret, frame = self.cap.read()
            if ret:
                filename = f"photo_{int(time.time())}.jpg"
                cv2.imwrite(filename, frame)
                self.text_display.append(f"ğŸ“¸ å·²ä¿å­˜ç…§ç‰‡ä¸ºï¼š{filename}")

    def stop_camera_preview(self):
        if self.cap:
            self.timer.stop()
            self.cap.release()
            self.cap = None
            self.video_label.clear()
            self.text_display.append("ğŸ“· ç›¸æœºå·²å…³é—­")


if __name__ == '__main__':
    app = QApplication(sys.argv)
    win = WhisperVoiceApp()
    win.show()
    sys.exit(app.exec_())
